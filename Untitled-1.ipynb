{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "# import pytesseract\n",
    "\n",
    "#pytesseract.pytesseract.tesseract_cmd = r'C:\\Users\\AL03207\\AppData\\Local\\Programs\\Tesseract-OCR\\tesseract.exe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 800x1024 2 text_bubbles, 1309.7ms\n",
      "Speed: 15.4ms preprocess, 1309.7ms inference, 4774.7ms postprocess per image at shape (1, 3, 800, 1024)\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a model\n",
    "model = YOLO('models/comic-speech-bubble-detector.pt')  # pretrained YOLOv8n model\n",
    "\n",
    "# Run batched inference on a list of images\n",
    "results = model(['temp/jp_row.jpg'])  # return a list of Results objects\n",
    "\n",
    "# Process results list\n",
    "for result in results:\n",
    "    boxes = result.boxes  # Boxes object for bounding box outputs\n",
    "    masks = result.masks  # Masks object for segmentation masks outputs\n",
    "    keypoints = result.keypoints  # Keypoints object for pose outputs\n",
    "    probs = result.probs  # Probs object for classification outputs\n",
    "    #result.show()  # display to screen\n",
    "    result.save(filename='result.jpg') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "def find_textbox(image):\n",
    "# Load image, grayscale, Gaussian blur, adaptive threshold\n",
    "#image = cv2.imread('temp/temp/1.png')\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    blur = cv2.GaussianBlur(gray, (9,9), 0)\n",
    "    thresh = cv2.adaptiveThreshold(blur,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV,11,30)\n",
    "\n",
    "    # Dilate to combine adjacent text contours\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (9,9))\n",
    "    dilate = cv2.dilate(thresh, kernel, iterations=4)\n",
    "\n",
    "    # Find contours, highlight text areas, and extract ROIs\n",
    "    cnts = cv2.findContours(dilate, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cnts = cnts[0] if len(cnts) == 2 else cnts[1]\n",
    "\n",
    "    ROI_number = 0\n",
    "    for c in cnts:\n",
    "        area = cv2.contourArea(c)\n",
    "        if area > 10000:\n",
    "            (x,y,w,h) =cv2.boundingRect(c)\n",
    "            cv2.rectangle(image, (x, y), (x + w, y + h), (36,255,12), 3)\n",
    "            cv2.imshow('d',image)\n",
    "            # ROI = image[y:y+h, x:x+w]\n",
    "            # cv2.imwrite('ROI_{}.png'.format(ROI_number), ROI)\n",
    "            # ROI_number += 1\n",
    "            return cv2.boundingRect(c)\n",
    "    return None\n",
    "\n",
    "\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from string import ascii_letters\n",
    "import textwrap\n",
    "\n",
    "def get_font_size(textarea, text, font_name, pixel_gap = 11):\n",
    "    text_width = int(textarea[0])\n",
    "    text_height = int(textarea[1])\n",
    "    \n",
    "    for point_size in range(5, 90):\n",
    "        wrapped_text = []\n",
    "        font = ImageFont.truetype(font_name, point_size)\n",
    "                \n",
    "        avg_char_width = sum(font.getbbox(char)[2] for char in ascii_letters) / len(ascii_letters)\n",
    "        max_char_height = max(font.getbbox(char)[3] - font.getbbox(char)[1] for char in ascii_letters)\n",
    "        \n",
    "        # Translate this average length into a character count\n",
    "        max_char_count = int( (text_width) / avg_char_width)\n",
    "        text = textwrap.fill(text=text, width=max_char_count)\n",
    "        num_line = len(text.splitlines())\n",
    "        \n",
    "        wrapped_text.append(text)\n",
    "        \n",
    "        if (max_char_height * num_line) + (pixel_gap * (num_line + 1)) >= text_height:\n",
    "            \n",
    "            point_size = point_size - 1\n",
    "            text = wrapped_text[-1]\n",
    "            \n",
    "            # print(\"\\n --> SIZE: \", point_size)\n",
    "            break\n",
    "        \n",
    "    return text, point_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "c:\\ProgramData\\Anaconda3\\envs\\kaggle\\lib\\site-packages\\transformers\\models\\vit\\feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#japanese\n",
    "from transformers import pipeline\n",
    "\n",
    "ocr_jp_pipe = pipeline(\"image-to-text\", model=\"models/kha-white/manga-ocr-base\")\n",
    "translation_jp_pipe = pipeline(\"translation\", model=\"models/Helsinki-NLP/opus-mt-ja-en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "て め ー の 仕 事 だ! It's because of you!\n",
      "It's\n",
      "b eca\n",
      "use\n",
      "of\n",
      "you!\n"
     ]
    }
   ],
   "source": [
    "image = cv2.imread('temp/jp_row.jpg')\n",
    "balloons = 1\n",
    "for x,y,w,h in boxes.xywh.numpy():\n",
    "    xmin = int(x - w/2)\n",
    "    ymin = int(y - h/2)\n",
    "    xmax = int(x + w/2)\n",
    "    ymax = int(y + h/2)\n",
    "    cropped_image = image[ymin:ymax,xmin:xmax]\n",
    "    cv2.imwrite(f'temp/temp/{balloons}.png', cropped_image) \n",
    "    cropped_pil_image = Image.fromarray(cropped_image)\n",
    "    cropped_pil_image.show()\n",
    "    generated_text = ocr_jp_pipe(cropped_pil_image)[0]['generated_text']\n",
    "    translated_text = translation_jp_pipe(generated_text)[0]['translation_text']\n",
    "    print(generated_text,translated_text)\n",
    "    window_x,window_y,window_w,window_h = find_textbox(cropped_image)\n",
    "    wrapped,font_size = get_font_size((window_w,window_h),translated_text,'man.ttf')\n",
    "    print(wrapped)\n",
    "    draw = ImageDraw.Draw(cropped_pil_image)\n",
    "    draw.rectangle((window_x,window_y,window_x+window_w,window_y+window_h), fill = \"white\")\n",
    "    font = ImageFont.truetype(r\"man.ttf\", font_size)\n",
    "\n",
    "    x_center = int(window_w / 2) + window_x\n",
    "    y_center = int(window_h/ 2) + window_y\n",
    "\n",
    "    draw.text((x_center, y_center), wrapped, font=font, fill=\"black\", anchor=\"mm\",align='center')\n",
    "    cropped_pil_image.save(f\"temp/{balloons}c.png\")\n",
    "    image[ymin:ymax,xmin:xmax] = cv2.cvtColor(np.array(cropped_pil_image), cv2.COLOR_RGB2BGR)\n",
    "    cv2.imshow('ff',image)\n",
    "    balloons += 1\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
